<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>iflowfor8hours.info</title>
    <link>http://localhost/tags/pipelines/index.xml</link>
    <description>Recent content on iflowfor8hours.info</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>2016, matt urbanski</copyright>
    <atom:link href="http://localhost/tags/pipelines/index.xml" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Migrating From a Static Ubuntu to Containerized Infrastructure</title>
      <link>http://localhost/post/2016/2016-12-06-migrating-to-containers/</link>
      <pubDate>Wed, 07 Dec 2016 00:00:00 +0000</pubDate>
      
      <guid>http://localhost/post/2016/2016-12-06-migrating-to-containers/</guid>
      <description>&lt;p&gt;The time has come to move off my statically hosted ubuntu box on Digital Ocean and on to a more modern and containerized stack.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Why?&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;My Digital Ocean box was 32-bit Ubuntu, and I had not done much in terms of maintaining it in a while. I considered upgrading it, but the configuration was a &lt;em&gt;work of art&lt;/em&gt; and my work with CoreOS and containers in my dayjob inspired me to move to something more modern. I didn&amp;rsquo;t want to go the Ansible plus static infrastructure route, since I have been migrating clients off that for a few years now. I wanted to run a few other applications in containers as well so I thought this would be a good starting point. I have done this type of work frequently for entire enterprises, so this was the obvious choice. I&amp;rsquo;m documenting it because it was a fun exercise and others might benefit from it. Here is the existing infrastructure and pipeline:&lt;/p&gt;

&lt;p&gt;Github -&amp;gt; Codeship Pipeline to hugo build, smoke test, then rsync artifacts -&amp;gt; Digital Ocean box with NGINX and Let&amp;rsquo;s Encrypt SSL&lt;/p&gt;

&lt;p&gt;Simple setup. I didn&amp;rsquo;t really like the rsync in the pipeline, or NGINX being non-configuration managed, but it worked and was comfortable.&lt;/p&gt;

&lt;p&gt;Here is the desired state:&lt;/p&gt;

&lt;p&gt;Github push -&amp;gt; something Hugo build -&amp;gt; something Container packaging -&amp;gt; Ship to containerized host -&amp;gt; Decomission previous nginx container.&lt;/p&gt;

&lt;p&gt;I also wanted to put let&amp;rsquo;s encrypt into its own container for bonus points, and so that I could run the same artifact locally and remotely.&lt;/p&gt;

&lt;p&gt;I got started by doing a basic local nginx container that mounted my generated hugo source by mounting it. This is for my quick iterative development workflow.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;  #!/bin/bash
  hugo --cleanDestinationDir --baseURL http://localhost/
  chmod -R 777 public
  docker stop iflowfor8hours-nginx-sidecar
  docker rm -f iflowfor8hours-nginx-sidecar
  docker run --name iflowfor8hours-nginx-sidecar -v &amp;quot;$PWD&amp;quot;/public:/usr/share/nginx/html -v &amp;quot;$PWD&amp;quot;/dev/nginx.conf:/etc/nginx/nginx.conf:ro -p 80:80 nginx:alpine
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;The &lt;code&gt;chmod -R 777&lt;/code&gt; is ugly, but I&amp;rsquo;m not concerned with anything getting broken since this is generated code and only used locally.&lt;/p&gt;

&lt;p&gt;The next phase involved creating the container that I would use in production. I didn&amp;rsquo;t want to bake the let&amp;rsquo;s encrypt stuff into the container because I felt that it violated the concept of small, single purpose containers. I wrote a minimal &lt;code&gt;Dockerfile&lt;/code&gt; first and built it by hand.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;FROM nginx:alpine
COPY public /usr/share/nginx/html
RUN chown -R nginx:nginx /usr/share/nginx/html
COPY dev/nginx.conf /etc/nginx/nginx.conf
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I&amp;rsquo;m using nginx:alpine because I was considering using &lt;a href=&#34;https://hyper.sh&#34;&gt;hyper.sh&lt;/a&gt; to host my containers, and still might. I wanted to keep it as small as possible since this is only going to be serving static html. This was another reason for the added complexity cost of putting let&amp;rsquo;s encrypt in another container.&lt;/p&gt;

&lt;p&gt;I then wrote the script for building the content and the container. I figured I might as well do this now, as the CI system will need some kind of entrypoint. This also helped me iterate quickly on my local box.&lt;/p&gt;

&lt;pre&gt;&lt;code&gt;#!/bin/bash
# Create a production and non-prod BASEURL environment variable in pipeline configuration

if [ -z &amp;quot;${BASEURL}&amp;quot; ]; then 
    HUGO_BASEURL=&#39;http://localhost/&#39;
else 
    HUGO_BASEURL=${BASEURL}
fi

env HUGO_BASEURL=${HUGO_BASEURL} hugo --cleanDestinationDir
docker build -t iflowfor8hours:blog .
echo docker run --name bakedblog -p 80:80 iflowfor8hours:blog
&lt;/code&gt;&lt;/pre&gt;

&lt;p&gt;I played with that for a bit, and everything works as expected. Now I need to get lets encrypt working. Since certs are free and I already own my domain, I can afford to have an https enabled staging environment. I now needed to decide on a docker hosting environment and platform. The easiest way to get up and running with containerized infrastructure is CoreOS, hands down. I spun up a coreos box on DO and then reconfigured my DNS to point it at.&lt;/p&gt;

&lt;p&gt;The next phase will be to setup the pipeline and finally deploy my containers to the newly provisioned container host.&lt;/p&gt;

&lt;p&gt;I also plan on migrating to IPv6 when the hosts are setup properly.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>The path to production and overengineering</title>
      <link>http://localhost/post/2016/2016-05-26-pipelines-and-overengineering/</link>
      <pubDate>Thu, 26 May 2016 00:00:00 +0000</pubDate>
      
      <guid>http://localhost/post/2016/2016-05-26-pipelines-and-overengineering/</guid>
      <description>

&lt;h4 id=&#34;the-path-to-production-is-beset-on-all-sides-by-the-inequity-of-the-good-intentioned-selfish-and-tyrrany-of-overengineering-ezekiel-25-17&#34;&gt;&lt;em&gt;&amp;ldquo;The path to production is beset on all sides by the inequity of the good-intentioned selfish and tyrrany of overengineering.&amp;rdquo;&lt;/em&gt; Ezekiel 25:17&lt;/h4&gt;

&lt;p&gt;The core of many development organizations is a desire to innovate technologically and maintain quality. A key factor of keeping up with the cutting edge is to accelerate development and maximize the productivity of your teams, which is often wrapped in the implementation of a build and deploy pipeline.&lt;/p&gt;

&lt;p&gt;Development teams need a pipeline that automatically runs tests, runs builds, maintains artifacts, configures applications, and manages deployments and promotions through their environments. These are the functional requirements of a pipeline. The acceptance criteria are that development is accelerated, and developer and operational experience of deployments are consistent and high quality. Core functionality of the pipeline is ensuring artifacts go from environment to environment reliably and repeatably so that risk is reduced. It is also a powerful tool for maintaining the discipline and health of the project when used properly.&lt;/p&gt;

&lt;h3 id=&#34;scope-creep-and-value-propositions&#34;&gt;Scope creep and value propositions&lt;/h3&gt;

&lt;p&gt;There are other requirements that may creep in such as tracking bugs and releases, integrating operational metrics from the application, adding business metrics around revenue or market events, and tracing quality or other characteristics from their version control system or other sources. These are &lt;em&gt;nice-to-haves&lt;/em&gt; in the first version of your pipeline and should be summarily sent to the backlog before discussions get too detailed.&lt;/p&gt;

&lt;p&gt;Many companies design their pipelines around their development environment, citing specifics about their processes or technologies that require attention. These are technology-based decisions around build systems, testing frameworks, and integration with internal systems of record for releases. I caution that these are considerations for after the pipeline has proven to provide some value.&lt;/p&gt;

&lt;p&gt;The value proposition of a pipeline begins to fade into the background as the technical details and toolset of the pipeline are decided upon. This is a slippery slope in any project, but especially toxic in infrastructure projects, where the customers and stakeholders of the project are often ill-defined or self-referential. By this I mean that the needs of internal customers, especially development teams are particularly difficult to extract.&lt;/p&gt;

&lt;h4 id=&#34;the-ideal-pipeline-is-an-unopinionated-orchestration-of-independent-tools-workflows-and-environmental-constraints-whose-output-is-configured-running-software&#34;&gt;&lt;em&gt;The ideal pipeline is an unopinionated orchestration of independent tools, workflows, and environmental constraints whose output is configured, running software.&lt;/em&gt;&lt;/h4&gt;

&lt;p&gt;The purpose of a pipeline is to continuously deliver high-quality software. As the complexity of the software or number of components increases, the CI pipeline should reduce complexity by having an established and repeatable path to production.&lt;/p&gt;

&lt;h3 id=&#34;mitigating-the-risks-of-overengineering&#34;&gt;Mitigating the risks of overengineering&lt;/h3&gt;

&lt;p&gt;As developers, we often strive to build the most generic and flexible system possible, to enable other developers to make responsible decisions for themselves. After all, they know their software better and are therefore poised to make the best decisions for the code. The ideal pipeline is an unopinionated orchestration of independent tools, workflows, and environmental constraints whose output is working, running software.&lt;/p&gt;

&lt;p&gt;This quote by Joel Moses encapsulates the risks of overengineering very well. An interesting piece of related trivia is that the ◇ character allowed multiple statements per line of text; it was APL&amp;rsquo;s semicolon.&lt;/p&gt;

&lt;h4 id=&#34;apl-is-like-a-beautiful-diamond-flawless-beautifully-symmetrical-but-you-can-t-add-anything-to-it-if-you-try-to-glue-on-another-diamond-you-don-t-get-a-bigger-diamond-lisp-is-like-a-ball-of-mud-add-more-and-it-s-still-a-ball-of-mud-it-still-looks-like-lisp&#34;&gt;&lt;em&gt;APL is like a beautiful diamond - flawless, beautifully symmetrical. But you can&amp;rsquo;t add anything to it. If you try to glue on another diamond, you don&amp;rsquo;t get a bigger diamond. Lisp is like a ball of mud. Add more and it&amp;rsquo;s still a ball of mud - it still looks like Lisp.&lt;/em&gt;&lt;/h4&gt;

&lt;p&gt;As experienced DevOps practicioners and developers, this is what we want. A loosely-coupled system that provides the abstractions to do just about anything in respect to building, testing, and releasing software. Jenkins and TeamCity are CI servers, built around the concepts of projects, artifacts, workspaces, and nodes. However, this abstraction is very limited and the concepts are inherently stateful, whereas with CD tools that understand these principles, developers are freed from CI-only abstractions.&lt;/p&gt;

&lt;p&gt;GoCD and concourse.ci take a higher-level conceptual view and allows users to express the complex relationships of dependent pipelines, determine the order in which artifacts must be built, and understand triggers based on their context. They use a small number of general concepts to encapsulate the intention and capture the value of the pipeline. Resources, jobs, and tasks with inputs and outputs are the primitives used to describe pipelines. This is actually far simpler and less error prone than the &lt;a href=&#34;https://larrycuban.wordpress.com/2010/06/08/the-difference-between-complicated-and-complex-matters/&#34;&gt;&lt;strong&gt;complicated&lt;/strong&gt; nature&lt;/a&gt; of plugins, frameworks, languages, and tools that a CI server has to deal with. The CD-aware toolchain expresses simple abstractions using the above concepts, pushing the complexity that CI is traditionally responsible for, to developers and users of the pipeline. This is where a strong conceptual model and framework of what your pipeline should accomplish is necessary for success.&lt;/p&gt;

&lt;h3 id=&#34;migration-of-complexity&#34;&gt;Migration of Complexity&lt;/h3&gt;

&lt;p&gt;It is important to note that this shift in where the complexity lies is the key consideration in different build and deployment pipeline paradigms. There is a critical designation between runtime complexity and compile-time complexity. How that is handled is paramount to a successful CD strategy.&lt;/p&gt;

&lt;p&gt;The widespread use of containers and microservices has accelerated the pattern of moving compile-time complexity on to the developer and away from the operations team far past the humble beginnings of &lt;a href=&#34;http://12factor.net&#34;&gt;Heroku&amp;rsquo;s 12 factor&lt;/a&gt; app philosophy. Conversely, this pushes runtime complexity (scaling, monitoring, security, redundancy) further away from the developer and onto the operations team. Developers may see this as losing control of their applications, and in some ways it is a sacrifice of freedom for safety. The operations team now needs to have service discovery working reliably, good configuration management, and well-abstracted data layers for other applications to consume. A very high level of operational maturity is required for this to work, let alone to begin paying dividends.&lt;/p&gt;

&lt;p&gt;Creating a good foundation is critical, however a balance of usefulness to developers and graceful operational orthodoxy must be struck and that balance is only known by you in your environment.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>